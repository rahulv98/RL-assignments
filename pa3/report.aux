\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}SMDP Q-Learning}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Learned Value functions with SMDP Q-Learning}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.1}Goal- $G_{1}$ with Random start state in room-1}{1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Q-Values for $G_{1}$ with random start state for option a)Up, b)Right.}}{1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Q-Values for $G_{1}$ with random start state for option a)Down, b)Left, c)Clockwise, d)Anti Clockwise.}}{2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces a)Value Function Plot b)Learnt Policy(x - not explored; (0-5) option as explained above) for Goal - $G_{1}$ which is marked as green with random starting state}}{2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.2}Goal- $G_{2}$ with Random start state in room-1}{3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Q-Values for $G_{2}$ with random start state for option a)Up, b)Right, c)Down, d)Left, e)Clockwise, f)Anti Clockwise.}}{3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces a)Value Function Plot b)Learnt Policy(x - not explored; (0-5) option as explained above) for Goal - $G_{2}$ which is marked as green with random starting state}}{4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.3}Goal- $G_{1}$ with Fixed start state in room-4}{4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Q-Values for $G_{1}$ with fixed start state for option a)Up, b)Right, c)Down, d)Left}}{4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Q-Values for $G_{1}$ with fixed start state for option a)Clockwise, b)Anti Clockwise.}}{5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces a)Value Function Plot b)Learnt Policy(x - not explored; (0-5) option as explained above) for Goal - $G_{1}$ which is marked as green with fixed starting state.}}{5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.4}Goal- $G_{2}$ with Fixed start state in room-4}{5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Q-Values for $G_{2}$ with fixed start state for option a)Up, b)Right.}}{5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Q-Values for $G_{2}$ with fixed start state for option a)Down, b)Left, c)Clockwise, d)Anti Clockwise.}}{6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces a)Value Function Plot b)Learnt Policy(x - not explored; (0-5) option as explained above) for Goal - $G_{2}$ which is marked as green with fixed starting state}}{6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Learnt Value functions with Intra Option Q-Learning}{7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.1}Goal- $G_{1}$ with Random start state in room-1}{7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces a)Value Function Plot b)Learnt Policy(x - not explored; (0-5) option as explained above) for Goal - $G_{1}$ which is marked as green with random starting state}}{7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.2}Goal- $G_{2}$ with Random start state in room-1}{7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces a)Value Function Plot b)Learnt Policy(x - not explored; (0-5) option as explained above) for Goal - $G_{2}$ which is marked as green with random starting state}}{7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.3}Goal- $G_{1}$ with Fixed start state in room-4}{8}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces a)Value Function Plot b)Learnt Policy(x - not explored; (0-5) option as explained above) for Goal - $G_{1}$ which is marked as green with fixed starting state}}{8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.4}Goal- $G_{2}$ with Fixed start state in room-4}{8}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces a)Value Function Plot b)Learnt Policy(x - not explored; (0-5) option as explained above) for Goal - $G_{2}$ which is marked as green with fixed starting state}}{8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Learning Curves and Observations}{9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Plot Comparing Average steps taken by the agent with different starting conditions and learning algorithms for goal a)$G_{1}$ b)$G_{2}$ with epsiodes(x-axis) in log scale.}}{9}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Deep RL}{10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Implementation Details}{10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Results and Best Hyperparameter Values}{10}\protected@file@percent }
\newlabel{fig: DQN Learning Curve}{{2.2}{10}}
\newlabel{fig: DQN Learning Curve}{{2.2}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces a)Learning Curve for DQN on CartPole-v0 environment averaged over 4 independent runs(different seeds) trained for 500 episodes. b) Test rewards obtained by the learnt agent over 20 episodes.}}{10}\protected@file@percent }
\newlabel{tab:table1}{{2.2}{10}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Final set of Hyperparameters which worked best for the DQN.}}{10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Observations}{11}\protected@file@percent }
\newlabel{fig: DQN Learning Curve with varying mini batch size}{{2.3}{11}}
\newlabel{fig: DQN Learning Curve with varying learning rate}{{2.3}{11}}
\newlabel{fig: DQN Learning Curve with varying starting epsilon}{{2.3}{11}}
\newlabel{fig: DQN Learning Curve}{{2.3}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces a)Learning Curve for DQN on CartPole-v0 environment trained for 500 episodes with varying a) Mini Batch Size, b) Learning Rate c) Starting Epsilon d) Replay Memory size}}{11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Removal of Target Network and Experience Replay}{12}\protected@file@percent }
\newlabel{fig: DQN Learning Curve without target network}{{2.4}{12}}
\newlabel{fig: DQN Learning Curve without replay memory.}{{2.4}{12}}
\newlabel{fig: DQN Learning Curve without replay memory and target network}{{2.4}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Learning Curve for DQN on CartPole-v0 environment a)without target network b) without replay memory, c) without both replay memory and target network averaged over 4 independent runs trained for 500 episodes.}}{12}\protected@file@percent }
