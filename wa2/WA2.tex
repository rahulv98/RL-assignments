\documentclass[solution,addpoints,12pt]{exam}
\printanswers
\usepackage{amsmath,amssymb}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage[colorlinks=true, linkcolor=blue, urlcolor=blue]{hyperref}
\newcommand{\RP}{\ensuremath{\mathsf{RP}}}
\newcommand{\expect}[1]{\ensuremath{\mathbb{E}[#1]}}
\newcommand{\dx}{\mathrm{d}x}
\usepackage{graphicx}
\begin{document}

\hrule
\vspace{1mm}
\noindent 
\begin{center}
{\Large CS6700 : Reinforcement Learning} \\
{\large Written Assignment \#2}
\end{center}
%\hfill Release date: 21 Jan, 2017, 12:00 pm}
\vspace{1mm}
\noindent 
{Deadline: ?}

\vspace{2mm}
\hrule

{\small

\begin{itemize}\itemsep0mm
\item This is an individual assignment. Collaborations and discussions are strictly
prohibited.
\item Be precise with your explanations. Unnecessary verbosity will be penalized.
\item Check the Moodle discussion forums regularly for updates regarding the assignment.
\item \textbf{Please start early.}

\end{itemize}
}

\hrule

\vspace{3mm}
\noindent {\sc Author :} RAHUL. V \\[1mm]
\noindent {\sc Roll Number :} ME16B171 \\
\hrule


\begin{questions}
\question[3]
Consider a bandit problem in which the policy parameters are mean $ \mu$ and variance $\sigma$ of normal distribution according to which actions are selected. Policy is defined as $ \pi(a;\mu,\sigma^2) = \frac{1}{\sqrt{2\pi\sigma}}e^-\frac{(a-\mu)^2}{2\sigma^2}$. Derive the parameter update conditions according to the REINFORCE procedure (assume baseline is zero).

\begin{solution}
Given policy is :
\begin{equation}
     \pi(a;\mu,\sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}}e^-\frac{(a-\mu)^2}{2\sigma^2}\notag
\end{equation} 
Update rule according to REINFORCE method is:

\begin{equation}
\theta_{t+1} \xleftarrow{} \theta_{t} + \alpha_{t} \cdot (r_{t} - b_{t}) \cdot \left.\frac{\partial \ln{\pi(a; \theta)}}{\partial \theta}\right\vert_{\theta_{t}}
\end{equation}
Here the policy parameters are: $\theta = [\mu, \sigma^2]$ \\
Computing the partial differential of policy w.r.t $\theta$
\begin{equation}
    \frac{\partial \ln{\pi(a)}}{\partial \mu} = \frac{a - \mu}{\sigma^2}
\end{equation}
\begin{equation}
    \frac{\partial \ln{\pi(a)}}{\partial \sigma} = \frac{(a - \mu)^2}{\sigma^3} - \frac{1}{\sigma}
\end{equation}
Update conditions are:
\begin{equation}
    \begin{bmatrix}
    \mu_{t+1} \\
    \sigma_{t+1}
    \end{bmatrix}
    \xleftarrow{}
    \begin{bmatrix}
    \mu_{t} \\
    \sigma_{t}
    \end{bmatrix}
    + \alpha_{t} \cdot r_{t} \cdot
    \begin{bmatrix}
       \frac{a - \mu_{t}}{\sigma_{t}^2} \\
      \frac{(a - \mu_{t})^2}{\sigma_{t}^3} - \frac{1}{\sigma_{t}}
    \end{bmatrix}
\end{equation}
\end{solution}
\question[6]
 Let us consider the effect of approximation on policy search and value function based methods. Suppose that a policy gradient method uses a class of policies that do not contain the optimal policy; and a value function based method uses a function approximator that can represent the values of the policies of this class, but not that of the optimal policy.
 \begin{enumerate}[label=(\alph*)]
     \question[2]  Why would you consider the policy gradient approach to be better than the value function based approach?
     \begin{solution}
        Policy gradient approach directly optimizes for policy unlike value based methods where they learn a value function and then infer a policy from it. Hence policy gradient methods tend to be more stable and less prone to failure.\\
        Policy gradient methods can also learn stochastic policies where as value function based methods can only represent deterministic policies. Also in high dimensional spaces and continuous action spaces value function based methods are highly inefficient as they need to optimize each time over the action space for action selection 
     \end{solution}
     \question[2]  Under what circumstances would the value function based approach be better than the policy gradient approach?
     \begin{solution}
        Most of the value function based approach are off-policy methods and policy gradient methods are on-policy methods. When the environment is expensive(for example a real environment where a failure would incur huge loses or the environment model is computationally expensive) we would want to make maximum use of the collected data. Which is only possible with off-policy methods. In these circumstances certain value function based methods are better than policy gradient approaches.
     \end{solution}
     \question[2]  Is there some circumstance under which either of the method can find the optimal policy?
     \begin{solution}
     When the state and action spaces are small, both the methods could possibly approximate the optimal policy accurately.
     \end{solution}
 
 \end{enumerate}
 

\question[4] Answer the following questions with respect to the DQN algorithm:
\begin{itemize}
    \question [2] When using one-step TD backup, the TD target is $R_{t+1}+\gamma V(S_{t+1},\theta)$ and the update to the neural network parameter is as follows:\\
    \begin{equation}
        \Delta \theta=\alpha(R_{t+1}+\gamma V(S_{t+1},\theta)-V(S_{t},\theta))\nabla_{\theta}V(S_{t},\theta)
    \end{equation}
    Is the update correct ? Is any term missing ? Justify your answer
    \begin{solution}
    No, the update is not correct as it doesn't use a target network and experience replay which are very important for the DQN to stabilize. \\
    The correct update is :
    \begin{equation}
        \Delta \theta=\alpha \mathbb{E}_{(s,a,r,s')\sim D}(r+\gamma V(s',\theta^{-})-V(s,\theta))\nabla_{\theta}V(s,\theta)
    \end{equation}
    Here $D$ is the experience replay buffer and $\theta_{-}$ are the target network parameters.    
    \end{solution}
    \question [2] Describe the two ways discussed in class to update the parameters of target network. Which one is better and why?
    \begin{solution}
    The two ways to update parameters of target networks are:
    \begin{enumerate}
     \item The parameters of the Q network are copied to the target network periodically after every \textbf{C} steps 
     \item Soft Update : The parameters of the target network are slowly moved towards the parameters of Q network after each step. \\
     \begin{equation}
      \theta^{-} = \tau \theta + (1 - \tau) \theta^{-}  \hspace{16px} \text{where} \hspace{10px} \tau << 1 
     \end{equation}
    \end{enumerate}

    The soft update is better as it is more stable compared to the first update because the target is changing very slowly.
    
    \end{solution}
\end{itemize}

\question[4] Experience replay is vital for stable training of DQN.
\begin{parts}
    \part[2] What is the role of the experience replay in DQN?
    \begin{solution}
    Experience replay is used to address the non-stationary and highly correlation between consequent samples obtained by interacting with the environment. Whereas to prove convergence of a deep neural network we need the samples to be uncorrelated and stationary. By storing the experiences in a replay buffer and sampling uniformly from the buffer making it close to i.i.d sampling and improves convergence.   
    \end{solution}
    \part[2] Consequent works in literature sample transitions from the experience replay, in proportion to the TD-error. Hence, instead of sampling transitions using a uniform-random strategy, higher TD-error transitions are sampled at a higher frequency. Why would such a modification help?
    \begin{solution}
    This method is called Prioritised Experience Replay. By sampling at higher frequency from transitions with high TD-error, we are giving more importance to cases where the estimates are poor compared to when the estimates are close the target network. Hence we train the DQN more on transitions with high TD-errors. this would help in quicker and more effective learning compared to uniform-random sampling.
    \end{solution}
\end{parts}

\question[3] We discussed two different motivations for actor-critic algorithms: the original motivation was as an extension of reinforcement comparison, and the modern motivation is as a variance reduction mechanism for policy gradient algorithms. Why is the original version
of actor-critic not a policy gradient method?
\begin{solution}
\begin{itemize}
 \item \textbf{Reinforcement Comparison:} 
 In this method we update the preferences of each state-action pair by comparing the return get with a baseline and the actions are picked based on these prefences.   
 \begin{equation}
  p_{t+1}(s_{t}, a_{t}) = p_{t}(s_{t}, a_{t}) + \beta [G_{t} - b_{t}]
 \end{equation}

 \item \textbf{Policy gradient:}
 In this method we evaluate the gradient of the expected return and update the parameters of the policy.
 \begin{equation}
  \theta_{t+1} = \theta_{t} + \alpha A^{\pi}(s_{t},a_{t}) \nabla_{\theta} \log \pi(s_{t},a_{t};\theta)
  \end{equation}
\end{itemize}
    In the reinforcement comparison method, the $G_{t} - b_{t}$ can be replaced by $\delta_{t}$ (TD - error) and extended into an actor-critic by learning the value function along with the policy. As there is no policy gradient involved in this version, it is not a policy gradient method.
\end{solution}


\question[4]
This question requires you to do some \href{https://arxiv.org/abs/cs/9905014}{additional reading}.  Dietterich specifies certain conditions for safe-state abstraction for the MaxQ framework.  I had mentioned in class that even if we do not use the MaxQ value function decomposition, the hierarchy provided is still useful.  So, which of the safe-state abstraction conditions are still necessary when we do not use value function decomposition?
\begin{solution}
Dietterich specified \textbf{5} conditions for safe-state abstraction for the MaxQ framework.

\begin{enumerate}
 \item Max Node Irrelevance
 \item Leaf Irrelevance
 \item Result Distribution Irrelevance
 \item Termination
 \item Shielding
\end{enumerate}

We can make use of the MaxQ graph without using value function decomposition. However the 5 conditions are for the complete MaxQ framework i.e. MaxQ graph (hierarchy) and MaxQ value function decomposition.  \textbf{Max Node Irrelevance} and \textbf{Leaf Irrevelance} safe-state abstraction condtions are still necessary when ew don't use value function decomposition as these are required to reduce state variables in a subtask.
\end{solution}

\question[3] Consider the problem of solving continuous control tasks using Deep Reinforcement Learning.

\begin{parts}
    \part[2] Why can simple discretization of the action space not be used to solve the problem? In which exact step of the DQN training algorithm is there a problem and why?
    \begin{solution}
    
    Simple discretization of action space cannot be used because the number of actions increases exponentially with the number of degrees of freedom. For example, a 7 degree of freedom human arm with the coarsest discretization $a_{i} \in \{-k,0,k\}$ for each joint leads to an action space with dimensionality\\
    $3^{7} = 2187$.\\
    In DQN, to find the TD target we need to get the max Q value over the action space which in a continuous space requires iterative optimisation process at each step, even if the action space is discretised, we still need to book-keep or evaluate the Q value for a high dimensional action space.
    \end{solution}
    
    \part[1] How is exploration ensured in the DDPG algorithm?
    \begin{solution}
    If the action space is discrete we can ensure exploration by using $\epsilon - greedy$ algorithm. If the action space is continuous, we can use an exploration policy $\mu'$ by adding noise sampled from a noise process $\mathcal{N}$ to our actor policy.
    \begin{equation}
        \mu'(s_{t}) = \mu(s_{t}|\theta_{t}^{\mu}) + \mathcal{N} 
    \end{equation}
    $\mathcal{N}$ can be chosen to suit the environment.
    \end{solution}
\end{parts}


\question[3] Option discovery has entailed using heuristics, the most popular of which is to identify bottlenecks. Justify why bottlenecks are useful sub-goals. Describe scenarios in which a such a heuristic could fail.
\begin{solution}
a bottleneck is a region in the agent’s observation space that the agent tends to visit frequently on successful paths to a goal but not on unsuccessful paths. Let's consider the example of 4 room grid world, if we don't use bottlenecks as sub goals, the agent would spend a lot of time exploring the initial room before it moves to another room which would delay the learning process. However if we used bottlenecks as sub-goals to discover the option we could speed up the learning process. \\
Using the same example, if there exists another goal state with higher reward in the far corner of the same initial room, and we are using options based on bottlenecks, the agent would take a long time learn that as the option is hindering exploration in the same room. Hence it fails.
\end{solution}

\end{questions}

\end{document}
